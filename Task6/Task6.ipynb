{
  "metadata": {
    "name": "Task6",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Фильтры для нахождения фильмов"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nsearch_genres \u003d \u0027Comedy\u0027\nyear_from \u003d 2000\nyear_to \u003d 2000\nregexp \u003d \u0027\u0027\ntop_n \u003d 0\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n\napt install unzip\n# download\nrm -rf /zeppelin/seed/ml-latest-small\nmkdir -p /zeppelin/seed\nwget -O dataset-small.zip \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\" --no-check-certificate\nunzip -j dataset-small.zip -d /zeppelin/seed/ml-latest-small\n\n\nrm -f /zeppelin/seed/ml-latest-small/links.csv\nrm -f /zeppelin/seed/ml-latest-small/tags.csv\nrm -f /zeppelin/seed/ml-latest-small/README.txt\nrm -f dataset-small.zip\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport re\n\nmovies \u003d sc.textFile(\"file:///zeppelin/seed/movies.csv\")\nratings \u003d sc.textFile(\"file:///zeppelin/seed/ratings.csv\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark \n\ndef parse_year(name):\n    year_index \u003d name.rfind(\u0027(\u0027)\n    try:\n        year \u003d int(name[year_index + 1:year_index + 5])\n    except ValueError:\n        year \u003d 0\n        year_index \u003d len(name)\n    return year, year_index\n    \ndef split_line(line):\n     return line[0:line.find(\u0027,\u0027)], \\\n           line[line.find(\u0027,\u0027) + 1:line.rfind(\u0027,\u0027)].replace(\u0027\"\u0027, \u0027\u0027), \\\n           line[line.rfind(\u0027,\u0027) + 1:len(line)].replace(\u0027\\r\u0027, \u0027\u0027)\n    \ndef normalize_movies(line):\n    try:\n        movie_id, title, genres \u003d split_line(line)\n        year, year_index \u003d parse_year(title)\n        movie_id \u003d int(movie_id)\n        title \u003d title[0:year_index - 1]\n        return (movie_id, (title, year, genres))\n    except:\n        return (None, (None, None, None))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Нормализуем фильмы в формат (movie_id, (title, year, genres))"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nrdd_normalized_movies \u003d movies.map(normalize_movies)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrdd_normalized_movies.take(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Фильтруем файл фильмов на корректность и по введеным фильтрам"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nrdd_filtered_movies \u003d rdd_normalized_movies.filter(lambda p :(\n    lambda title, year, genres:(\n        title !\u003d None and year !\u003d None and genres !\u003d None and\n        search_genres in genres and\n        year_from \u003c\u003d year \u003c\u003d year_to and\n        re.search(regexp, title)\n    ))(*p[1]))"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nrdd_filtered_movies.take(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndef normalize_ratings(line):\n    try:\n        _, movie_id, rating, _ \u003d line.split(\u0027,\u0027)\n        return (int(movie_id), (float(rating), 1))\n    except:\n        return (None, (None, None))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Нормализуем файл рейтингов в формат (movie_id, (rating, 1))"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nrdd_normalized_ratings \u003d ratings.map(normalize_ratings)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nrdd_normalized_ratings.take(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Считаем рейтинги для каждого id_movie и возвращаем в виде tuple(movie_id, (sum_ratings, count_ratings))"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nrdd_counted_ratings \u003d rdd_normalized_ratings\\\n    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\\\n    .filter(lambda x: x[0] !\u003d None)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nrdd_counted_ratings.take(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrdd_filtered_movies.take(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Объединяем таблицу фильмов и рейтингов по полю movie_id и считаем средний рейтинг"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\nrdd_movies_rating \u003d rdd_filtered_movies\\\n    .join(rdd_counted_ratings.map(lambda x: (x[0], (round(x[1][0] / x[1][1], 2),))))\\\n    .map(lambda x: (x[0], (x[1][0] + x[1][1])))\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrdd_movies_rating.take(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nСортируем фильмы по рейтингу по убыванию и году по возрастанию"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrdd_sorted1 \u003d rdd_movies_rating.sortBy(lambda x: -x[1][3])\nrdd_sorted1.take(20)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Сортируем фильмы по рейтинги и году"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport heapq\n# rdd_sorted \u003d rdd_movies_rating.sortBy(lambda x: (-x[1][3], x[1][1]))\ndef key(kv):\n    return kv[1][3], -kv[1][1]\n    \n# def merge(a, b):\n#     return heapq.nlargest(10, a + b, key)\n    \n# rdd_movies_rating.mapPartitions(lambda iter: heapq.nlargest(10, iter, key)).reduce((lambda a, b: heapq.nlargest(10, a + b, key)))\nif top_n \u003d\u003d 0:\n        top_n \u003d 100000\n        \ndef topIterator(iterator):\n    \n    yield heapq.nlargest(top_n, iterator, key\u003dkey)\n\ndef merge(a, b):\n    return heapq.nlargest(top_n, a + b, key\u003dkey)\n\nrdd_sorted \u003d rdd_movies_rating.mapPartitions(topIterator).reduce(merge)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Делаем JSON like формат"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrdd_result \u003d rdd_sorted.flatMap(lambda x: [(genre, [{\u0027title\u0027:x[1][0], \u0027year\u0027: x[1][1], \u0027rating\u0027:x[1][3]}]) for genre in x[1][2].split(\u0027|\u0027)])\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrdd_result.take(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Объединяем фильмы по жанрам и возвращаем JSON"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nrdd_result_json \u003d rdd_result.reduceByKey(lambda x, y: x + y).map(lambda x: {\u0027genre\u0027: x[0], \u0027movies\u0027: x[1]})"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Сохраняем результат"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nrdd_result_json.saveAsTextFile(\u0027/zeppelin/seed/result\u0027)"
    }
  ]
}