{
 "metadata": {
  "name": "Task6",
  "kernelspec": {
   "language": "scala",
   "name": "spark2-scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фильтры для нахождения фильмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "search_genres = 'Comedy'\n",
    "year_from = 2000\n",
    "year_to = 2000\n",
    "regexp = ''\n",
    "top_n = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%sh\n\nrm -rf /zeppelin/seed/result"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%sh\n\napt install unzip\n# download\nrm -rf /zeppelin/seed/ml-latest-small\nmkdir -p /zeppelin/seed\nwget -O dataset-small.zip \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\" --no-check-certificate\nunzip -j dataset-small.zip -d /zeppelin/seed/ml-latest-small\n\n\nrm -f /zeppelin/seed/ml-latest-small/links.csv\nrm -f /zeppelin/seed/ml-latest-small/tags.csv\nrm -f /zeppelin/seed/ml-latest-small/README.txt\nrm -f dataset-small.zip\n"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\nimport re\n\nmovies = sc.textFile(\"file:///zeppelin/seed/movies.csv\")\nratings = sc.textFile(\"file:///zeppelin/seed/ratings.csv\")"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark \n\ndef parse_year(name):\n    year_index = name.rfind('(')\n    try:\n        year = int(name[year_index + 1:year_index + 5])\n    except ValueError:\n        year = 0\n        year_index = len(name)\n    return year, year_index\n    \ndef split_line(line):\n     return line[0:line.find(',')], \\\n           line[line.find(',') + 1:line.rfind(',')].replace('\"', ''), \\\n           line[line.rfind(',') + 1:len(line)].replace('\\r', '')\n    \ndef normalize_movies(line):\n    try:\n        movie_id, title, genres = split_line(line)\n        year, year_index = parse_year(title)\n        movie_id = int(movie_id)\n        title = title[0:year_index - 1]\n        return (movie_id, (title, year, genres))\n    except:\n        return (None, (None, None, None))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Нормализуем фильмы в формат (movie_id, (title, year, genres))"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\n\nrdd_normalized_movies = movies.map(normalize_movies)"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\nrdd_normalized_movies.take(5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Фильтруем файл фильмов на корректность и по введеным фильтрам"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\n\nrdd_filtered_movies = rdd_normalized_movies.filter(lambda p :(\n    lambda title, year, genres:(\n        title != None and year != None and genres != None and\n        search_genres in genres and\n        year_from <= year <= year_to and\n        re.search(regexp, title)\n    ))(*p[1]))"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\n\nrdd_filtered_movies.take(5)"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\n\ndef normalize_ratings(line):\n    try:\n        _, movie_id, rating, _ = line.split(',')\n        return (int(movie_id), (float(rating), 1))\n    except:\n        return (None, (None, None))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Нормализуем файл рейтингов в формат (movie_id, (rating, 1))"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\n\nrdd_normalized_ratings = ratings.map(normalize_ratings)"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\n\nrdd_normalized_ratings.take(5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Считаем рейтинги для каждого id_movie и возвращаем в виде tuple(movie_id, (sum_ratings, count_ratings))"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\n\nrdd_counted_ratings = rdd_normalized_ratings\\\n    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\\\n    .filter(lambda x: x[0] != None)"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\n\nrdd_counted_ratings.take(5)"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\nrdd_filtered_movies.take(5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Объединяем таблицу фильмов и рейтингов по полю movie_id и считаем средний рейтинг"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\n\n\nrdd_movies_rating = rdd_filtered_movies\\\n    .join(rdd_counted_ratings.map(lambda x: (x[0], (round(x[1][0] / x[1][1], 2),))))\\\n    .map(lambda x: (x[0], (x[1][0] + x[1][1])))\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\nrdd_movies_rating.take(5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Сортируем фильмы по рейтинги и году"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\nimport heapq\n# rdd_sorted = rdd_movies_rating.sortBy(lambda x: (-x[1][3], x[1][1]))\n# rdd_sorted = sc.parallelize((rdd_movies_rating.top(top_n, key=key)))\n# rdd_movies_rating.mapPartitions(lambda iter: heapq.nlargest(10, iter, key)).reduce((lambda a, b: heapq.nlargest(10, a + b, key)))\n# rdd_sorted = sc.parallelize(rdd_movies_rating.mapPartitions(topIterator).reduce(merge))\ndef key(kv):\n    return kv[1][3], -kv[1][1]\n\nif top_n == 0:\n    top_n = 100000\n    \ndef topIterator(iterator):\n    yield heapq.nlargest(top_n, iterator, key=key)\n\ndef merge(a, b):\n    return heapq.nlargest(top_n, a + b, key=key)\n        \nrdd_sorted = sc.parallelize(rdd_movies_rating.mapPartitions(topIterator).reduce(merge))\n\n\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\nrdd_sorted.take(5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Делаем JSON like формат"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\nrdd_result = rdd_sorted.flatMap(lambda x: [(genre, [{'title':x[1][0], 'year': x[1][1], 'rating':x[1][3]}]) for genre in x[1][2].split('|')])\n"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\nrdd_result.take(5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Объединяем фильмы по жанрам и возвращаем JSON"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\n\nrdd_result_json = rdd_result.reduceByKey(lambda x, y: x + y).map(lambda x: {'genre': x[0], 'movies': x[1]})"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Сохраняем результат"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "%pyspark\n\nrdd_result_json.saveAsTextFile('/zeppelin/seed/result')"
  }
 ]
}